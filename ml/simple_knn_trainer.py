# ml/simple_knn_trainer.py - –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è

import numpy as np
from typing import Tuple, Dict, List, Optional
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc
from sklearn.preprocessing import StandardScaler
import pickle
import os
from datetime import datetime

from ml.feature_extractor import FeatureExtractor
from config import MODELS_DIR, MIN_TRAINING_SAMPLES

class SimpleKNNTrainer:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è kNN –º–æ–¥–µ–ª–∏ –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è"""
    
    def __init__(self, user_id: int):
        self.user_id = user_id
        self.feature_extractor = FeatureExtractor()
        self.scaler = StandardScaler()
        self.model = None
        self.best_params = {}
        self.training_stats = {}
        
    def prepare_training_data(self, positive_samples: List) -> Tuple[np.ndarray, np.ndarray]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –±–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è"""
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤
        X_positive = self.feature_extractor.extract_features_from_samples(positive_samples)
        n_positive = len(X_positive)
        
        if n_positive < MIN_TRAINING_SAMPLES:
            raise ValueError(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –æ–±—Ä–∞–∑—Ü–æ–≤: {n_positive}, –Ω—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º {MIN_TRAINING_SAMPLES}")
        
        print(f"üìä –ê–ù–ê–õ–ò–ó –í–ê–®–ò–• –î–ê–ù–ù–´–•:")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤: {n_positive}")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–∞—à–∏ –¥–∞–Ω–Ω—ã–µ
        mean_positive = np.mean(X_positive, axis=0)
        std_positive = np.std(X_positive, axis=0)
        
        print(f"   –í—Ä–µ–º—è —É–¥–µ—Ä–∂–∞–Ω–∏—è: {mean_positive[0]*1000:.1f}¬±{std_positive[0]*1000:.1f} –º—Å")
        print(f"   –í—Ä–µ–º—è –º–µ–∂–¥—É –∫–ª–∞–≤–∏—à–∞–º–∏: {mean_positive[2]*1000:.1f}¬±{std_positive[2]*1000:.1f} –º—Å")
        print(f"   –°–∫–æ—Ä–æ—Å—Ç—å: {mean_positive[4]:.1f}¬±{std_positive[4]:.1f} –∫–ª/—Å–µ–∫")
        
        # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –±–æ–ª–µ–µ –†–ê–ó–õ–ò–ß–ò–ú–´–• –Ω–µ–≥–∞—Ç–∏–≤–æ–≤
        X_negative = self._generate_diverse_negatives(X_positive, factor=1.2)
        n_negative = len(X_negative)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–¥–µ–ª–∏–º–æ—Å—Ç—å
        self._check_separability(X_positive, X_negative)
        
        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        X = np.vstack([X_positive, X_negative])
        y = np.hstack([np.ones(n_positive), np.zeros(n_negative)])
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        X_normalized = self.scaler.fit_transform(X)
        
        print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã: {n_positive} –≤–∞—à–∏—Ö, {n_negative} –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö")
        return X_normalized, y
    
    def _generate_diverse_negatives(self, X_positive: np.ndarray, factor: float = 1.2) -> np.ndarray:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ë–û–õ–ï–ï –°–õ–û–ñ–ù–´–• –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è"""
        n_samples = len(X_positive)
        n_negatives = int(n_samples * factor)
        
        mean = np.mean(X_positive, axis=0)
        std = np.std(X_positive, axis=0)
        
        # ‚úÖ –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
        std = np.maximum(std, mean * 0.3)  # –ë—ã–ª–æ 0.15, —Å—Ç–∞–ª–æ 0.3
        
        negatives = []
        
        print(f"üîß –°–û–ó–î–ê–ù–ò–ï –ë–û–õ–ï–ï –°–õ–û–ñ–ù–´–• –ù–ï–ì–ê–¢–ò–í–û–í:")
        
        # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1 - –ù–ï–ú–ù–û–ì–û —Ä–∞–∑–ª–∏—á–∞—é—â–∏–µ—Å—è (40% –≤–º–µ—Å—Ç–æ 25%)
        similar_count = int(n_negatives * 0.4)
        print(f"   {similar_count} –ø–æ—Ö–æ–∂–∏—Ö –Ω–æ —Ä–∞–∑–ª–∏—á–∞—é—â–∏—Ö—Å—è...")
        for _ in range(similar_count):
            # –°–æ–∑–¥–∞–µ–º –æ–±—Ä–∞–∑—Ü—ã –ë–õ–ò–ó–ö–ò–ï –∫ –≤–∞—à–∏–º, –Ω–æ —Å –Ω–µ–±–æ–ª—å—à–∏–º–∏ –æ—Ç–ª–∏—á–∏—è–º–∏
            base_idx = np.random.randint(0, len(X_positive))
            sample = X_positive[base_idx].copy()
            
            # –ò–∑–º–µ–Ω—è–µ–º —Ç–æ–ª—å–∫–æ 1-2 –ø—Ä–∏–∑–Ω–∞–∫–∞ –Ω–∞ –Ω–µ–±–æ–ª—å—à—É—é –≤–µ–ª–∏—á–∏–Ω—É
            features_to_change = np.random.choice(6, size=np.random.randint(1, 3), replace=False)
            
            for feat_idx in features_to_change:
                # –ù–ï–ë–û–õ–¨–®–ò–ï –∏–∑–º–µ–Ω–µ–Ω–∏—è: 80%-120% –æ—Ç –∏—Å—Ö–æ–¥–Ω–æ–≥–æ
                change_factor = np.random.uniform(0.8, 1.2)
                sample[feat_idx] *= change_factor
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à–æ–π —à—É–º
            noise = np.random.normal(0, std * 0.2)  # –£–º–µ–Ω—å—à–∏–ª–∏ —à—É–º
            sample += noise
            sample = np.maximum(sample, mean * 0.3)
            negatives.append(sample)
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –£–º–µ—Ä–µ–Ω–Ω–æ –º–µ–¥–ª–µ–Ω–Ω—ã–µ (20%)
        slow_count = int(n_negatives * 0.2)
        print(f"   {slow_count} —É–º–µ—Ä–µ–Ω–Ω–æ –º–µ–¥–ª–µ–Ω–Ω—ã—Ö...")
        for _ in range(slow_count):
            sample = mean.copy()
            # –£–º–µ—Ä–µ–Ω–Ω–æ –º–µ–¥–ª–µ–Ω–Ω–æ (–Ω–µ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ!)
            sample[0] *= np.random.uniform(1.3, 1.8)    # dwell time
            sample[2] *= np.random.uniform(1.4, 2.2)    # flight time
            sample[4] *= np.random.uniform(0.6, 0.8)    # speed
            sample[5] *= np.random.uniform(1.3, 1.8)    # total time
            
            # –£–º–µ—Ä–µ–Ω–Ω–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
            sample[1] *= np.random.uniform(1.2, 2.0)    # dwell std
            sample[3] *= np.random.uniform(1.2, 2.0)    # flight std
            
            noise = np.random.normal(0, std * 0.3)
            sample += noise
            sample = np.maximum(sample, mean * 0.2)
            negatives.append(sample)
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: –£–º–µ—Ä–µ–Ω–Ω–æ –±—ã—Å—Ç—Ä—ã–µ (20%)
        fast_count = int(n_negatives * 0.2)
        print(f"   {fast_count} —É–º–µ—Ä–µ–Ω–Ω–æ –±—ã—Å—Ç—Ä—ã—Ö...")
        for _ in range(fast_count):
            sample = mean.copy()
            # –£–º–µ—Ä–µ–Ω–Ω–æ –±—ã—Å—Ç—Ä–æ
            sample[0] *= np.random.uniform(0.6, 0.8)    # dwell time
            sample[2] *= np.random.uniform(0.5, 0.7)    # flight time
            sample[4] *= np.random.uniform(1.2, 1.6)    # speed
            sample[5] *= np.random.uniform(0.6, 0.8)    # total time
            
            # –ù–∏–∑–∫–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
            sample[1] *= np.random.uniform(0.4, 0.8)    # dwell std
            sample[3] *= np.random.uniform(0.4, 0.8)    # flight std
            
            noise = np.random.normal(0, std * 0.3)
            sample += noise
            sample = np.maximum(sample, mean * 0.2)
            negatives.append(sample)
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 4: –ó–∞–ø–æ–ª–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–µ–µ—Å—è —Å–ª—É—á–∞–π–Ω—ã–º–∏ –≤–∞—Ä–∏–∞—Ü–∏—è–º–∏
        remaining_count = n_negatives - similar_count - slow_count - fast_count
        print(f"   {remaining_count} —Å–ª—É—á–∞–π–Ω—ã—Ö –≤–∞—Ä–∏–∞—Ü–∏–π...")
        for _ in range(remaining_count):
            # –ë–µ—Ä–µ–º —Å–ª—É—á–∞–π–Ω—ã–π –æ–±—Ä–∞–∑–µ—Ü –∫–∞–∫ –æ—Å–Ω–æ–≤—É
            base_idx = np.random.randint(0, len(X_positive))
            sample = X_positive[base_idx].copy()
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ, –Ω–æ —É–º–µ—Ä–µ–Ω–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
            for i in range(len(sample)):
                change_factor = np.random.uniform(0.7, 1.4)  # –ë–æ–ª–µ–µ —É–º–µ—Ä–µ–Ω–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
                sample[i] *= change_factor
            
            # –£–º–µ—Ä–µ–Ω–Ω—ã–π —à—É–º
            noise = np.random.normal(0, std * 0.4)
            sample += noise
            sample = np.maximum(sample, mean * 0.1)
            negatives.append(sample)
        
        negatives_array = np.array(negatives)
        
        print(f"   –°–æ–∑–¥–∞–Ω–æ: {len(negatives_array)} —É–º–µ—Ä–µ–Ω–Ω–æ —Ä–∞–∑–ª–∏—á–∞—é—â–∏—Ö—Å—è –Ω–µ–≥–∞—Ç–∏–≤–æ–≤")
        return negatives_array
    
    def _check_separability(self, X_positive: np.ndarray, X_negative: np.ndarray):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–¥–µ–ª–∏–º–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–æ–≤ —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ–º –æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–∏"""
        from sklearn.metrics.pairwise import euclidean_distances
        
        # –†–∞—Å—Å—Ç–æ—è–Ω–∏—è –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏
        distances = euclidean_distances(X_positive, X_negative)
        min_dist = np.min(distances)
        mean_dist = np.mean(distances)
        
        # –í–Ω—É—Ç—Ä–∏–∫–ª–∞—Å—Å–æ–≤—ã–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è (–≤–∞—à–∏ –¥–∞–Ω–Ω—ã–µ)
        if len(X_positive) > 1:
            intra_distances = euclidean_distances(X_positive, X_positive)
            intra_distances = intra_distances[intra_distances > 0]
            mean_intra = np.mean(intra_distances)
            
            separation_ratio = mean_dist / mean_intra if mean_intra > 0 else float('inf')
        else:
            separation_ratio = mean_dist
            mean_intra = 0
        
        print(f"üîç –ü–†–û–í–ï–†–ö–ê –†–ê–ó–î–ï–õ–ò–ú–û–°–¢–ò:")
        print(f"   –ú–∏–Ω. —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏: {min_dist:.2f}")
        print(f"   –°—Ä–µ–¥–Ω–µ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏: {mean_dist:.2f}")
        if mean_intra > 0:
            print(f"   –°—Ä–µ–¥–Ω–µ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –≤–Ω—É—Ç—Ä–∏ –∫–ª–∞—Å—Å–∞: {mean_intra:.2f}")
            print(f"   –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Ä–∞–∑–¥–µ–ª–∏–º–æ—Å—Ç–∏: {separation_ratio:.2f}")
            
            # ‚úÖ –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï –û –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–ò
            if separation_ratio > 20.0:
                print(f"‚ö†Ô∏è  –í–´–°–û–ö–ò–ô –†–ò–°–ö –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–Ø! –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç > 20")
                print(f"    –ù–µ–≥–∞—Ç–∏–≤—ã —Å–ª–∏—à–∫–æ–º –¥–∞–ª–µ–∫–∏ –æ—Ç –≤–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö")
            elif separation_ratio > 10.0:
                print(f"‚ö†Ô∏è  –°–†–ï–î–ù–ò–ô –†–ò–°–ö –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–Ø! –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç > 10")  
            elif separation_ratio > 5.0:
                print(f"‚úÖ –ü—Ä–∏–µ–º–ª–µ–º–∞—è —Ä–∞–∑–¥–µ–ª–∏–º–æ—Å—Ç—å (–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç 5-10)")
            else:
                print(f"‚ö†Ô∏è  –í–æ–∑–º–æ–∂–Ω–æ —Å–ª–∞–±–∞—è —Ä–∞–∑–¥–µ–ª–∏–º–æ—Å—Ç—å (–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç < 5)")
                
            # –ï—Å–ª–∏ —Å–ª–∏—à–∫–æ–º —Ö–æ—Ä–æ—à–∞—è —Ä–∞–∑–¥–µ–ª–∏–º–æ—Å—Ç—å - —ç—Ç–æ –ø–ª–æ—Ö–æ!
            if separation_ratio > 15.0:
                print(f"üö® –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø: –£–º–µ–Ω—å—à–∏—Ç—å —Ä–∞–∑–ª–∏—á–∏—è –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏!")
        else:
            print(f"‚úÖ –ë–∞–∑–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–π–¥–µ–Ω–∞")
    
    def train_user_model(self, positive_samples: List) -> Tuple[bool, float, str]:
        """–û—Å–Ω–æ–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è"""
        try:
            print(f"üéØ –ù–ê–ß–ê–õ–û –£–õ–£–ß–®–ï–ù–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {self.user_id}")
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            X, y = self.prepare_training_data(positive_samples)
            
            # ‚úÖ –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ï—â–µ –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö –≤ —Ç–µ—Å—Ç
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.5, random_state=42, stratify=y  # 50% –≤ —Ç–µ—Å—Ç!
            )

            print(f"üìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:")
            print(f"   –û–±—É—á–µ–Ω–∏–µ: {len(X_train)} –æ–±—Ä–∞–∑—Ü–æ–≤ ({np.sum(y_train == 1)} –≤–∞—à–∏—Ö, {np.sum(y_train == 0)} –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö)")
            print(f"   –¢–µ—Å—Ç: {len(X_test)} –æ–±—Ä–∞–∑—Ü–æ–≤ ({np.sum(y_test == 1)} –≤–∞—à–∏—Ö, {np.sum(y_test == 0)} –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö)")
            
            # –ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            best_params = self._optimize_hyperparameters(X_train, y_train)
            
            # –û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
            self.model = KNeighborsClassifier(**best_params)
            self.model.fit(X_train, y_train)
            
            # ‚úÖ –ß–µ—Å—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ
            test_accuracy, roc_data = self._evaluate_model(X_test, y_test)
            
            # ‚úÖ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–ª—è —á–µ—Å—Ç–Ω–æ—Å—Ç–∏
            cv_scores = cross_val_score(self.model, X_train, y_train, cv=5, scoring='accuracy')
            cv_mean = np.mean(cv_scores)
            cv_std = np.std(cv_scores)
            
            print(f"üìà –ö–†–û–°–°-–í–ê–õ–ò–î–ê–¶–ò–Ø:")
            print(f"   CV —Ç–æ—á–Ω–æ—Å—Ç—å: {cv_mean:.3f} ¬± {cv_std:.3f}")
            print(f"   –†–∞–∑–±—Ä–æ—Å: {cv_std:.3f} ({'–ù–∏–∑–∫–∏–π' if cv_std < 0.1 else '–í—ã—Å–æ–∫–∏–π'})")
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è
            self.training_stats = {
                'user_id': self.user_id,
                'training_samples': len(positive_samples),
                'total_samples': len(X),
                'train_samples': len(X_train),
                'test_samples': len(X_test),
                'best_params': best_params,
                'test_accuracy': test_accuracy,
                'cv_accuracy': cv_mean,
                'cv_std': cv_std,
                'training_date': datetime.now().isoformat(),
                **roc_data  # –î–æ–±–∞–≤–ª—è–µ–º ROC –¥–∞–Ω–Ω—ã–µ
            }
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            self._save_model()
            
            print(f"‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û:")
            print(f"   –¢–µ—Å—Ç–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {test_accuracy:.2%}")
            print(f"   CV —Ç–æ—á–Ω–æ—Å—Ç—å: {cv_mean:.2%}")
            
            return True, test_accuracy, f"–ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ —Å —Ç–æ—á–Ω–æ—Å—Ç—å—é {test_accuracy:.2%} (CV: {cv_mean:.2%})"
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")
            import traceback
            traceback.print_exc()
            return False, 0.0, f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {str(e)}"
    
    def _optimize_hyperparameters(self, X_train: np.ndarray, y_train: np.ndarray) -> Dict:
        """–ë–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        
        # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ë–æ–ª–µ–µ —Ä–∞–∑—É–º–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        param_grid = {
            'n_neighbors': range(3, min(15, len(X_train) // 8)),  # –£–≤–µ–ª–∏—á–∏–ª–∏ –º–∏–Ω–∏–º—É–º
            'weights': ['uniform', 'distance'],
            'metric': ['euclidean', 'manhattan', 'minkowski'],
            'p': [1, 2]  # –î–ª—è minkowski
        }
        
        # ‚úÖ –°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        
        grid_search = GridSearchCV(
            KNeighborsClassifier(),
            param_grid,
            cv=cv,
            scoring='accuracy',
            n_jobs=-1
        )
        
        grid_search.fit(X_train, y_train)
        
        print(f"üîß –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –ü–ê–†–ê–ú–ï–¢–†–û–í:")
        print(f"   –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {grid_search.best_params_}")
        print(f"   CV —Ç–æ—á–Ω–æ—Å—Ç—å: {grid_search.best_score_:.3f}")
        
        self.best_params = grid_search.best_params_
        return grid_search.best_params_
    
    def _evaluate_model(self, X_test: np.ndarray, y_test: np.ndarray) -> Tuple[float, Dict]:
        """–ß–µ—Å—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ —Å ROC –¥–∞–Ω–Ω—ã–º–∏"""
        y_pred = self.model.predict(X_test)
        y_proba = self.model.predict_proba(X_test)[:, 1]
        
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, zero_division=0)
        recall = recall_score(y_test, y_pred, zero_division=0)
        f1 = f1_score(y_test, y_pred, zero_division=0)
        
        print(f"üìä –ú–ï–¢–†–ò–ö–ò –ù–ê –¢–ï–°–¢–û–í–û–ô –í–´–ë–û–†–ö–ï:")
        print(f"   Accuracy: {accuracy:.3f}")
        print(f"   Precision: {precision:.3f}")
        print(f"   Recall: {recall:.3f}")
        print(f"   F1-score: {f1:.3f}")
        
        # ‚úÖ ROC –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞
        try:
            if len(np.unique(y_test)) > 1:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å –æ–±–∞ –∫–ª–∞—Å—Å–∞
                fpr, tpr, thresholds = roc_curve(y_test, y_proba)
                roc_auc = auc(fpr, tpr)
                
                roc_data = {
                    'precision': precision,
                    'recall': recall, 
                    'f1_score': f1,
                    'y_test': y_test.tolist(),
                    'y_proba': y_proba.tolist(),
                    'fpr': fpr.tolist(),
                    'tpr': tpr.tolist(),
                    'roc_auc': roc_auc
                }
                
                print(f"   ROC AUC: {roc_auc:.3f}")
            else:
                print("‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –í —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –∫–ª–∞—Å—Å!")
                roc_data = {
                    'precision': precision,
                    'recall': recall,
                    'f1_score': f1,
                    'y_test': y_test.tolist(),
                    'y_proba': y_proba.tolist()
                }
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ ROC: {e}")
            roc_data = {
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'y_test': y_test.tolist(),
                'y_proba': y_proba.tolist()
            }
        
        return accuracy, roc_data
    
    def predict(self, features: np.ndarray) -> Tuple[bool, float]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –æ—Ç–ª–∞–¥–∫–æ–π"""
        if self.model is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        features_scaled = self.scaler.transform(features.reshape(1, -1))
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
        proba = self.model.predict_proba(features_scaled)[0]
        confidence = proba[1] if len(proba) > 1 else proba[0]
        
        # ‚úÖ –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏
        base_threshold = 0.75
        if hasattr(self, 'training_stats'):
            test_acc = self.training_stats.get('test_accuracy', 0.85)
            cv_std = self.training_stats.get('cv_std', 0.1)
            
            # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–∞—è, —Å–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥
            if cv_std > 0.15:
                threshold = base_threshold - 0.1
            elif test_acc < 0.8:
                threshold = base_threshold - 0.05
            else:
                threshold = base_threshold
        else:
            threshold = base_threshold
        
        is_legitimate = confidence >= threshold
        
        print(f"üîç –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï:")
        print(f"   –ü—Ä–∏–∑–Ω–∞–∫–∏: {features}")
        print(f"   –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.3f}")
        print(f"   –ü–æ—Ä–æ–≥: {threshold:.3f}")
        print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç: {'–ü–†–ò–ù–Ø–¢' if is_legitimate else '–û–¢–ö–õ–û–ù–ï–ù'}")
        
        return is_legitimate, confidence
    
    def _save_model(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        model_path = os.path.join(MODELS_DIR, f"user_{self.user_id}_simple_knn.pkl")
        
        model_data = {
            'model': self.model,
            'scaler': self.scaler,
            'best_params': self.best_params,
            'training_stats': self.training_stats
        }
        
        with open(model_path, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
    
    @classmethod
    def load_model(cls, user_id: int) -> Optional['SimpleKNNTrainer']:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        model_path = os.path.join(MODELS_DIR, f"user_{user_id}_simple_knn.pkl")
        
        if not os.path.exists(model_path):
            return None
        
        try:
            with open(model_path, 'rb') as f:
                model_data = pickle.load(f)
            
            trainer = cls(user_id)
            trainer.model = model_data['model']
            trainer.scaler = model_data['scaler']
            trainer.best_params = model_data['best_params']
            trainer.training_stats = model_data.get('training_stats', {})
            
            return trainer
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
            return None
    
    def get_model_info(self) -> Dict:
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏"""
        return {
            'is_trained': self.model is not None,
            'best_params': self.best_params,
            'training_stats': self.training_stats
        }